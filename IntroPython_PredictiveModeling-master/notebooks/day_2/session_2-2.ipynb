{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2.2\n",
    "\n",
    "\n",
    "## Logistic regression\n",
    "- In the previous section we learned about linear regression, where we trained a model to make predictions about the output values $y$ of some input data $X$\n",
    "- Now, we'll learn about ** logistic regression **, where the goal is to make classification decisions given some input data\n",
    "    - For example, a lot of social media applications use classification models like logistic regression to predict whether a user will ** like ** üëç or ** dislike ** üëé a post\n",
    "    - This is an example of a ** binary classification problem **, where the output can only have one of two values\n",
    "    - For now we'll stick with binary problems, but later on we'll address how to generalize to multiple classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ** Objective: ** Given some input data $X$ and class associations $y\\epsilon\\{\\mathrm{class A, class B}\\}$, create and train a model that will predict the class of new data $X_{\\mathrm{new}}$\n",
    "- Immediately there's a problem, we need to attach a numerical value to the classes we're trying to predict, but classes don't have any intrinsic numerical values...\n",
    "- Let's arbitrarily choose one of our classes to have value '1' and the other class to have value '0'\n",
    "$$ \\mathrm{class A}\\rightarrow 0, \\mathrm{class B}\\rightarrow 1 $$\n",
    "- (We could have just named them 1 and 2, but 0 and 1 is the standard convention)\n",
    "\n",
    "\n",
    "\n",
    "- We could now perform a linear regression and declare all outputs with $\\hat{y}^{i}<0.5$ to be class 0, and all outputs with $\\hat{y}^{i}>=0.5$ to be of class 1\n",
    "- The following image shows the results of a linear regression that does just that\n",
    "![linear regression w/ binary data](./images/linreg_classification.png)\n",
    "- This isn't actually all that bad of an idea, but we can do better\n",
    "- Instead, let's devise an algorithm that yields a ** probabilistic interpretation ** of the outputs, where the output of each input is a *prediction* about the probability that the input belongs to class 1\n",
    "- In addition to that change, we should also rethink our choice of loss function; previously we used a loss function that was the sum of the residuals squared, but there's no reason to believe that this is the best cost function when our output predictions are probabilities from 0 to 1 and the actual outputs are binary random variables\n",
    "- Below is a picture comparing ** regression ** with ** classification **---notice that the output of the logistic regression model is a * probability *\n",
    "![lin reg vs log reg](./images/regression_classification.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Giving our model a probabilistic interpretation---the link function\n",
    "- In linear regression, we mapped the inputs to the outputs * via * the ** link function ** $\\hat{y}^{i}=\\vec{x}^{i}\\vec{\\beta}$\n",
    "- Now we would like our predictions $y$ to be probabilities, so we need a link function $f$ that restricts the output to between 0 and 1: $f\\left(\\vec{x}^{i}\\vec{\\beta}\\right)\\epsilon\\left[0,1\\right]$\n",
    "- One such function that squashes $\\vec{x}^{i}\\vec{\\beta}$ into the range $\\left[0,1\\right]$ is the ** logistic function **, plotted below\n",
    "\n",
    "$$ f\\left(\\vec{x}^{i}\\vec{\\beta}\\right)=P\\left(y^{i}=1\\right)=\\frac{1}{1+e^{-\\vec{x}^{i}\\vec{\\beta}}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def logistic_function(z):\n",
    "    return 1./(1+np.exp(-z))\n",
    "\n",
    "zs = np.linspace(-10,10,1000)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.plot(zs, logistic_function(zs), lw = 3)\n",
    "plt.plot([-10,10],[0.5,0.5], ls = '--', c = 'k')\n",
    "\n",
    "plt.xlim(-10,10)\n",
    "plt.ylim(-.1, 1.1)\n",
    "\n",
    "plt.title('Logistic function', size = 18)\n",
    "plt.xlabel('z', size = 16)\n",
    "plt.ylabel('f(z)', size = 16)\n",
    "plt.grid()\n",
    "plt.tick_params(labelsize = 16)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f\\left(\\vec{x}^{i}\\vec{\\beta}\\right)=P\\left(y^{i}=1\\right)=\\frac{1}{1+e^{-\\vec{x}^{i}\\vec{\\beta}}} $$\n",
    "- To understand the logistic function, its useful to consider three limiting cases\n",
    "    - ** $\\vec{x}^{i}\\vec{\\beta}\\gg 1$ **: This equation says for large $\\vec{x}^{i}\\vec{\\beta}$, the probability that data point $x_{i}$ belongs to class 1 is $P\\left(y^{i}=1\\right)\\rightarrow 1$\n",
    "    - ** $\\vec{x}^{i}\\vec{\\beta}\\ll -1$ **: $P\\left(y^{i}=1\\right)\\rightarrow 0$\n",
    "    - ** $\\vec{x}^{i}\\vec{\\beta}\\sim 0$ **: for $\\vec{x}^{i}\\vec{\\beta}=0$, $P\\left(y^{i}=1\\right)=0.5$; both classes are equally likely\n",
    "- The advantage of having our model's outputs be probabilities instead of a binary prediction of class 0 or 1 is it allows us to make appropriate decisions given the uncertainty\n",
    "- For instance, if a model used in a cancer diagnostic predicts that a patience only has a 5% chance of having cancer, it is still probably worthwhile to bring them in for more tests. If the prediction is that the patient only has a 0.001% chance of cancer, no such tests are necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross-entropy error, our new loss function for logistic regression\n",
    "- As mentioned before, because our model is predicting probabilites rather than actual outputs, it is useful to consider new loss functions\n",
    "- For logistic regression, we use the ** cross-entropy error ** loss function instead of the residual sum of squares function that we used for linear regression\n",
    "\n",
    "$$ \\mathrm{Error}=\\sum_{i=0}^{N-1}\\left[-y^{i}\\log P\\left(\\hat{y}^{i}\\right)-\\left(1-y^{i}\\right)\\log\\left(1-P\\left(\\hat{y}^{i}\\right)\\right)\\right] $$\n",
    "\n",
    "- In order to intuitively understand the error, consider three cases:\n",
    "    - $y^{i}=1$, but we predict $P\\left(y^{i}=1\\right)=0.01$; a poor prediction\n",
    "        - In this case, $y^{i}$ really belongs to class one, but we've given a strong prediction it belongs to class zero\n",
    "        - The second term in the cross-entropy error loss function disappears, and the error of this individual term is $-\\log 0.01=2$, a large error\n",
    "    - $y^{i}=1$ and we predict $P\\left(y^{i}=1\\right)=0.99$; a great prediction\n",
    "        - The error for this data point is $-\\log 0.99=0.004$, a very small error\n",
    "    - $y^{i}=0$ or $y^{i}=1$, and $P\\left(y^{i}=1\\right)=0.5$; a completely uncertain prediction\n",
    "        - In any case, the error is $-\\log 0.5=0.3$, a non-negligible but reasonable error\n",
    "\n",
    "- Here's a plot of the cross-entropy error of a single data point, for both cases where the class is 0 or 1\n",
    "- Notice the MASSIVE error associated with very very certain, wrong predictions\n",
    "\n",
    "![cross-entropy error](./images/cross-entropy_loss.png)\n",
    "\n",
    "- We have a loss function, great!\n",
    "- Just like with linear regression, our task is now to find the parameters $\\vec{\\beta}$ that minimize the loss function\n",
    "- This can be performed analytically or numerically, e.g. with gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### That's all there is to it, folks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <font color=\"red\">Student activity: Digit classification</font>\n",
    "- Let's use our knowledge of logistic regression to build a digit recognizing application!\n",
    "- We'll start by building a model that is able to detect whether a digit falls into one of two classes:\n",
    "    - 0: Digit ** is ** a 0\n",
    "    - 1: Digit ** is not ** a 0\n",
    "- This application doesn't sound totally useful now, but we'll use it as the starting point for a model that will discriminate between all digits!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building a zero/not zero classifier\n",
    "- The data is contained in the './digits/data.csv' file; load it\n",
    "- Take the first 80% of the file as training, last 20% as test data\n",
    "- Separate the inputs from the outputs, and convert the outputs as described above ($0\\rightarrow0$, $1-9\\rightarrow1$)\n",
    "    - Hint: Use `np.where(X_train == 0)` and `np.where(X_train != 0)` to get the indices of x = 0 and x != 0, respectively; then, use something like `X_train[indices] = 0 or 1`\n",
    "- Create, train, and predict a model on the test data\n",
    "    - Hint: Use the `sklearn.linear_model.LogisticRegression` class\n",
    "- Answer the follow-up questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Load the data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model\n",
    "\n",
    "##########################\n",
    "# Insert solution below! #\n",
    "##########################\n",
    "\n",
    "# Open full file\n",
    "file_path = '../data/digits/data_reduced_large.csv'\n",
    "full_data = np.genfromtxt(file_path, delimiter = ',', skip_header = 1)\n",
    "\n",
    "# Get number of samples, features\n",
    "N_total = 'TODO 1'\n",
    "N_train = 'TODO 2' # Get 80% of the length (must be an integer)\n",
    "\n",
    "N_test = N_total - N_train\n",
    "\n",
    "num_columns = 'TODO 3'\n",
    "K = num_columns - 1    # subtract 1 so we don't count the label as a feature\n",
    "width = int(K**.5)            # Convenience constant for image width\n",
    "\n",
    "\n",
    "# Split into train and test\n",
    "X_train = full_data[:N_train, 1:K+1] # Select the first 80% of data\n",
    "y_train = full_data[:N_train, 0]\n",
    "\n",
    "X_test = full_data['TODO 4'] # Select the remaining 20% of data\n",
    "y_test = full_data['TODO 5']\n",
    "\n",
    "\n",
    "# Set outputs (y) to binary type (0 -> Is a zero, 1-> Not a zero)\n",
    "chosen_digit = 0\n",
    "\n",
    "change_indices_train = np.where(y_train == chosen_digit)[0]\n",
    "keep_indices_train = np.where(y_train != chosen_digit)[0]\n",
    "y_train[change_indices_train] = 0\n",
    "y_train[keep_indices_train] = 1\n",
    "\n",
    "change_indices_test = 'TODO 6'\n",
    "keep_indices_train = 'TODO 7'\n",
    "y_test[change_indices_test] = 0\n",
    "y_test[keep_indices_train] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Create, train, and fit a model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Insert solution below! #\n",
    "##########################\n",
    "import time\n",
    "\n",
    "\n",
    "# Put your code between the two time.time() functions if you wish to benchmark the fitting time\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = sklearn.linear_model.LogisticRegression(C=1e10)    \n",
    "\n",
    "# TODO: Fit/train model\n",
    "\n",
    "# TODO: Make predictions\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "print('time elapsed (s):\\t\\t', t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Answer/complete the following</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"red\">What is the range of each feature? How many features are there? Based on these facts, what do you suspect the features represent? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Insert solution below! **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"red\">In the ** training set ** how many class 0s are there? Class 1s? Are the classes balanced?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Insert solution below! #\n",
    "##########################\n",
    "\n",
    "N_train_zeroes = 'TODO 1'    # Number of 0s\n",
    "N_train_ones = 'TODO 2'      # Number of non-0s\n",
    "\n",
    "print(N_train_zeroes, N_train_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the classes are not balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"red\">Plot three examples of class 0 and three examples of class 1; be sure to plot them in the two-dimensional form.</font>\n",
    "- Hints: \n",
    "    - Write a function that takes an input as its function argument and plots it.\n",
    "    - Use matplotlib's `imshow()` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Insert solution below! #\n",
    "##########################\n",
    "\n",
    "def PlotDigit(x):\n",
    "    plt.imshow(x.reshape(width, width), cmap = 'gray', interpolation = 'none', vmin = 0, vmax = 255)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Plot the 0s (also try looking at other examples by changing the slice)\n",
    "for i in np.where(y_train == 0)[0][:3]:\n",
    "    PlotDigit('TODO 1')\n",
    "    \n",
    "# Plot the 1s\n",
    "for i in np.where(y_train != 0)[0][:3]:\n",
    "    PlotDigit('TODO 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"red\">Calculate the ** total error rate **, ** false positive rate **, and ** false negative rate **</font>\n",
    "\n",
    "- A false positive occurs when $\\hat{y}^{i}=1$ but $y^{i}=0$\n",
    "- A false negative is when $\\hat{y}^{i}=0$ but $y^{i}=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Insert solution below! #\n",
    "##########################\n",
    "\n",
    "\n",
    "# False Positive rate\n",
    "test_set_labeled_zero = y_test[y_test == 0]\n",
    "predicted_labeled_zero y_test_predictions[y_test == 0]\n",
    "\n",
    "false_positive_rate = len(np.where(test_set_labeled_zero != predicted_labeled_zero)[0])/N_test\n",
    "\n",
    "# False Negative rate\n",
    "test_set_labeled_non_zero = 'TODO 1'\n",
    "predicted_labeled_non_zero 'TODO 2'\n",
    "\n",
    "false_negative_rate = len(np.where(test_set_labeled_non_zero != predicted_labeled_non_zero)[0])/N_test\n",
    "\n",
    "# Total error rate\n",
    "total_error_rate = 'TODO 3'\n",
    "\n",
    "print('Total Error', total_error_rate, 'FP', false_positive_rate, 'FN', false_negative_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color=\"red\">Find three examples of a ** false positive ** and two examples of ** false negatives **. Plot each. Print the probability associated with each prediction (hint: Look at the [sklearn docs for logistic regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to find the function for getting a prediction probability!). Is it evident why the model failed to accurately predict the class?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Insert solution below! #\n",
    "##########################\n",
    "\n",
    "false_positive_indices = np.where((y_test != y_test_predictions) & (y_test == 0))[0][:3]\n",
    "false_negative_indices = 'TODO 1'\n",
    "\n",
    "print('false positives (predicted not zero, actually is zero)')\n",
    "for index in false_positive_indices:\n",
    "    print(model.predict_proba(X_test[index]))\n",
    "    PlotDigit(X_test[index])\n",
    "    \n",
    "print ('false negatives (predicted zero, is not actually zero)')\n",
    "for index in false_negative_indices:\n",
    "    print(model.predict_proba(X_test[index]))\n",
    "    PlotDigit(X_test[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Beyond binary predictions---logistic regression with $>2$ classes\n",
    "- Cool, so we just figured out how to distinguish between 0s and non-0s\n",
    "- Great job! \n",
    "# üëçüëçüëçüëçüëçüëçüëçüëçüëçüëç\n",
    "- Adapting what we did above to classify the full range of digits 0-9 is actually not all that hard\n",
    "- There are a few ways of doing it, but we're going to go with a method called ** one-vs-rest ** logistic regression (OvR logistic regression)\n",
    "- How does it work?\n",
    "\n",
    "\n",
    "##### One-vs-rest logistic regression\n",
    "1. Train one model per class category; in this case, there will be 10 models for the 10 possible digits\n",
    "2. Each model is like the model we created above, but for a different number\n",
    "    - For instance, the classes for model 3 will be 'digit is 3' (1) and 'digit is not 3' (0)\n",
    "3. So, we train 10 models to predict whether a digit is or is not an example of that model's selected digit\n",
    "4. Finally, when making predictions on test data, we go with the digit belonging to the model that had the most confidence\n",
    "\n",
    "\n",
    "\n",
    "- Example:\n",
    "            \n",
    "| Model | Positive prediction probability            |\n",
    "|-------|--------------------------------------------|\n",
    "| 0     | .15                                        |\n",
    "| 1     | .10                                        |\n",
    "| 2     | .03                                        |\n",
    "| ** 3 **     | ** .89 **                                       |\n",
    "| 4     | .12                                        |\n",
    "| 5     | .66                                        |\n",
    "| 6     | .13                                        |\n",
    "| 7     | .10                                        |\n",
    "| 8     | .24                                        |\n",
    "| 9     | .70                                        |\n",
    "\n",
    "- Thus, for the above example because model 3 is the most confident that the input is positive, we predict that the digit is a 3\n",
    "\n",
    "##### Multiclass logistic regression with scikit-learn\n",
    "- The above describes what we ** would ** do if we were implementing this ourselves\n",
    "- Fortunately, sklearn is smart enough to do the hard work for us! We simply need to create a single sklearn.linear_model.LogisticRegression() model, and it will implement OvR logistic regression on its own by default, although other multiclassification approaches can be specified as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Building a complete digit recognizer\n",
    "- Load the data, but don't split it into training and testing; put it all in training data\n",
    "- Create and train a model to predict digits using sklearn.linear_model.LogisticRegression()\n",
    "- Test the model on ** 10 ** of your own hand written digits, 0-9\n",
    "- Write some digits on a piece of paper, snap a photo with a smart phone, and send to your e-mail\n",
    "- After downloading, run them through the preprocessing steps already coded-up below (you may have to slightly alter some of the parameters, read the notes in the cell)\n",
    "    - Hints:\n",
    "        - Draw on the white board, if you so desire\n",
    "        - Draw thick lines\n",
    "        - Make sure your paper/whiteboard is uniformly and well lit\n",
    "        - Make sure there are no other stray pen marks in a region around each hand drawn digit\n",
    "\n",
    "- Determine the accuracy on the 10 hand-written digits    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <font color=\"red\">Student activity: 0-9 Digit classification</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Load data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "# Python standard library\n",
    "import time\n",
    "\n",
    "# Scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model\n",
    "\n",
    "##########################\n",
    "# Insert solution below! #\n",
    "##########################\n",
    "file_path = '../data/digits/data_reduced_large.csv'\n",
    "full_data = 'TODO 1' # Use genfromtxt\n",
    "\n",
    "\n",
    "K = full_data.shape[1] - 1    # subtract 1 so we don't count the label as a feature\n",
    "width = int(K**.5)            # Convenience constant for image width\n",
    "\n",
    "\n",
    "\n",
    "# Split into inputs and outputs\n",
    "output_column = 0\n",
    "\n",
    "X = full_data['TODO 1']\n",
    "y = full_data['TODO 2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Create, train, and fit a model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Insert solution below! #\n",
    "##########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Put your code between the two time.time() functions if you wish to benchmark the fitting time\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "model = sklearn.linear_model.LogisticRegression(n_jobs = -1, C = 1e10)    # Create model\n",
    "# TODO: Fit/train model\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "print('time elapsed (s):\\t\\t', t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Hand-drawn digit (pen and paper)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def PreprocessImage(image_file_path):\n",
    "    # Set up fig\n",
    "    fig, axes = plt.subplots(3, 3, figsize = (12,12))\n",
    "    \n",
    "    custom_digit = plt.imread(image_file_path)[:,:,[0,1,2]]\n",
    "    \n",
    "    # Load in .png image\n",
    "    plt.sca(axes[0,0])\n",
    "    plt.imshow(custom_digit)\n",
    "    plt.title('raw image')\n",
    "    \n",
    "\n",
    "\n",
    "    # Remove alpha channel\n",
    "    custom_digit = custom_digit[:,:,[0,1,2]]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Rotate image\n",
    "    # This step may not be necessary; my phone always rotates images for some reason...\n",
    "    plt.sca(axes[0,1])\n",
    "    \n",
    "    custom_digit = np.rot90(custom_digit, axes = (1,0))\n",
    "    plt.imshow(custom_digit, cmap = 'gray')\n",
    "    plt.title('rotated image')\n",
    "\n",
    "\n",
    "    # Average RGB channels\n",
    "    plt.sca(axes[0,2])\n",
    "    \n",
    "    custom_digit = np.mean(custom_digit, axis = 2)\n",
    "    plt.imshow(custom_digit, cmap = 'gray')\n",
    "    plt.title('convert to grayscale')\n",
    "    \n",
    "\n",
    "\n",
    "    # Invert grays\n",
    "    plt.sca(axes[1,0])\n",
    "    \n",
    "    custom_digit = 255-custom_digit\n",
    "    plt.imshow(custom_digit, cmap = 'gray')\n",
    "    plt.title('invert grays')\n",
    "    \n",
    "\n",
    "\n",
    "    # Histogram\n",
    "    plt.sca(axes[1,1])\n",
    "    \n",
    "    plt.hist(custom_digit.flatten(), bins = 255)\n",
    "    plt.yscale('log', nonposy='clip')\n",
    "    \n",
    "    plt.title('intensity histogram')\n",
    "    \n",
    "\n",
    "\n",
    "    # Threshold background to black\n",
    "    \n",
    "    # Set the correct threshold to separate the dark background from the light foreground (the letters)\n",
    "    # The threshold should be between the very large background peak and the smaller peak in the middle\n",
    "    \n",
    "    plt.sca(axes[1,2])\n",
    "\n",
    "    threshold = 175\n",
    "    custom_digit[custom_digit < threshold] = 0\n",
    "    plt.imshow(custom_digit, cmap = 'gray')\n",
    "    \n",
    "    plt.title('threshold background to black')\n",
    "    \n",
    "    # Rescale\n",
    "    plt.sca(axes[2,0])\n",
    "    \n",
    "    custom_digit = custom_digit*255./np.max(custom_digit)\n",
    "    \n",
    "    plt.imshow(custom_digit, cmap = 'gray')\n",
    "    \n",
    "    plt.title('rescale brightness')\n",
    "\n",
    "\n",
    "    # Crop image\n",
    "    plt.sca(axes[2,1])\n",
    "    \n",
    "    \n",
    "    threshold = 1\n",
    "    digit_top_row = np.where(np.mean(custom_digit, axis = 1) > threshold)[0][0]\n",
    "    digit_bottom_row = np.where(np.mean(custom_digit, axis = 1) > threshold)[0][-1]\n",
    "    digit_left_column = np.where(np.mean(custom_digit, axis = 0) > threshold)[0][0]\n",
    "    digit_right_column = np.where(np.mean(custom_digit, axis = 0) > threshold)[0][-1]\n",
    "\n",
    "    center = [int((digit_bottom_row + digit_top_row)/2.), int((digit_right_column + digit_left_column)/2.)]\n",
    "\n",
    "    width = int(2.5*(digit_right_column - digit_left_column))\n",
    "\n",
    "    new_top_row = center[0] - int(width/2.)\n",
    "    new_bottom_row = new_top_row + width\n",
    "\n",
    "    new_left_column = center[1] - int(width/2.)\n",
    "    new_right_column = new_left_column + width\n",
    "\n",
    "\n",
    "    custom_digit = np.copy(custom_digit[new_top_row:new_bottom_row + 1, new_left_column:new_right_column + 1])\n",
    "    plt.imshow(custom_digit, cmap = 'gray')\n",
    "    \n",
    "    plt.title('crop')\n",
    "    \n",
    "    print('debugging info')\n",
    "    print(digit_top_row, digit_bottom_row, digit_left_column, digit_right_column, center, width)\n",
    "    \n",
    "\n",
    "    # Down sample image\n",
    "    plt.sca(axes[2,2])\n",
    "    custom_digit = scipy.misc.imresize(custom_digit, size = (28, 28))\n",
    "    plt.imshow(custom_digit, cmap = 'gray')\n",
    "    \n",
    "    plt.title('down sample')\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return custom_digit.flatten()\n",
    "\n",
    "\n",
    "# ~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~\n",
    "# Replace this string with the location of your own file!\n",
    "# ~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~!~\n",
    "\n",
    "custom_digit = PreprocessImage('./data/digits/hand-drawn_7_0.JPG')\n",
    "\n",
    "print(model.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Hand-drawn digit (digital)</font>\n",
    "- Open your favorite paint tool (ms paint, gimp, photoshop, etc.) and create a 28x28 pixel image\n",
    "- Paint the background black, draw your letter in white using the paint brush or air brush tool\n",
    "- Save and replace the following string to the location of your digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in .png image\n",
    "custom_digit = plt.imread('../data/digits/custom_digit.png')[:,:,[0,1,2]]\n",
    "\n",
    "# Remove alpha channel\n",
    "custom_digit = custom_digit[:,:,[0,1,2]]\n",
    "\n",
    "# Average RGB channels\n",
    "custom_digit = np.mean(custom_digit, axis = 2)\n",
    "\n",
    "# Rescale [0-1]->[0,255]\n",
    "custom_digit = custom_digit*255\n",
    "\n",
    "plt.imshow(custom_digit, cmap = 'gray', vmin = 0, vmax = 255, interpolation = 'none')\n",
    "plt.show()\n",
    "\n",
    "print(model.predict(custom_digit.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- In summary, we learned about ** logistic regression **, a generalized linear model that allows us to learn to make categorical predictions by learning on labelled data\n",
    "- ** Logistic regression ** is very similar to ** linear regression **, with only a few major differences\n",
    "    1. The data is * categorical *\n",
    "    2. The predictions are * probabilities *\n",
    "    3. Logistic regression uses the ** logistic function ** as its link function\n",
    "        - Logistic regression: \n",
    "        $$f\\left(\\vec{x}^{i}\\vec{\\beta}\\right)=\\frac{1}{1+e^{-\\vec{x}^{i}\\vec{\\beta}}}$$\n",
    "        - Linear regression:\n",
    "        $$f\\left(\\vec{x}^{i}\\vec{\\beta}\\right)=\\vec{x}^{i}\\vec{\\beta}$$\n",
    "    4. Logistic regression uses the ** cross-entropy error ** which has the form\n",
    "        - Logistic regression:\n",
    "        $$\\mathrm{Error}=\\Sigma_{i=0}^{N-1}\\left[-y^{i}\\log \\hat{y}^{i} - \\left(1-y^{i}\\right)\\log\\left(1-\\hat{y}^{i}\\right)\\right]$$\n",
    "        - Linear regression: (residual sum of squares)\n",
    "        $$\\mathrm{Error}=\\Sigma_{i=0}^{N-1}\\left(y^{i}-\\hat{y}^{i}\\right)^{2}$$\n",
    "        \n",
    "- Two useful quantities for assessing a logistic regression model are the ** false positive rate ** (rate of predicting class 1 when really class 0) and ** false negative rate ** (rate of predicting class 0 when really class 1)\n",
    "- ** One-versus-Rest (OvR) ** logistic regression can extend plain logistic regression to cover multiple classifications instead of binary classifications\n",
    "- Scikit-learn is an amazing tool for getting logistic regression classifiers trained and making predictions in a short amount of time; building our hand-written digit detector took ~30 minutes, and only ~5 minutes to train!\n",
    "\n",
    "### Data engineering\n",
    "- Another point that wasn't explicitly stated in this section was that model building and fitting is only the tip of the iceberg when it comes to data science problems\n",
    "- The modeling part of the code-base only took 3 lines and little effort:\n",
    "    - `model = sklearn.linear_model.LogisticRegression([parameters])`\n",
    "    - `model.fit(X, y)`\n",
    "    - `model.predict(X_new, y_new)`\n",
    "    \n",
    "- Compare this with the amount of code to load the data and preprocess it\n",
    "- Model building is a hugely important part of data science, but so is all the preprocessing that takes place!\n",
    "- Raw data is seldom, if ever, in a form that we can throw into a model without processing\n",
    "- 'Data engineering' is a huge part of every data science problem!\n",
    "- Python is an amazing tool for processing and analyzing data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
